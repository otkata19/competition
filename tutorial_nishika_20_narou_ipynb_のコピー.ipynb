{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_nishika_20_narou.ipynb のコピー",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otkata19/competition/blob/main/tutorial_nishika_20_narou_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpvxpeIbSN0E"
      },
      "source": [
        "## 小説家になろう ブクマ数予測 \\~”伸びる”タイトルとは？\\~ チュートリアル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rShkCrJSTD0"
      },
      "source": [
        "このnotebookは、Nishikaコンペティション [小説家になろう ブクマ数予測 \\~”伸びる”タイトルとは？\\~](https://www.nishika.com/competitions/21/summary) のチュートリアルです。\n",
        "\n",
        "コンペタイトルにもある通り、「小説タイトル」「小説あらすじ」といったテキストデータから得られる情報がブクマ数予測に効くか？というのが、1つのテーマです。\n",
        "\n",
        "そこで本notebookでは、特にテキストデータからの特徴量作成に焦点を当てます。<br>\n",
        "例えば、タイトルが短くまとまっている作品に良作品が多い、などの傾向があるかもしれません（「[無職転生](https://ncode.syosetu.com/n9669bk/)」「[俺は星間国家の悪徳領主！](https://ncode.syosetu.com/n1976ey/)」などが好きな私による全くの主観です）。\n",
        "\n",
        "もちろん、精度向上には構造化データによる特徴量作成も効果的ですので、色々試してみましょう！<br>\n",
        "（構造化データからの特徴量作成は、[こちらのチュートリアル](https://colab.research.google.com/drive/1uGESzJFtzbDwGSq9Jk1w4QhtrPjP5XqY?usp=sharing)などをご覧ください）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWybjYLP-GHu"
      },
      "source": [
        "| 要素 | 説明 |\n",
        "| ---- | ---- |\n",
        "|ncode | Nコード|\n",
        "|general_firstup | 初回掲載日 YYYY-MM-DD HH:MM:SSの形式|\n",
        "|title | 小説名|\n",
        "|story | 小説のあらすじ|\n",
        "|keyword | キーワード|\n",
        "|userid | 作者のユーザID(数値)|\n",
        "|writer | 作者名|\n",
        "|biggenre | 大ジャンル|\n",
        "|genre | ジャンル|\n",
        "|novel_type | 連載の場合は1、短編の場合は2|\n",
        "|end | 短編小説と完結済小説は0となっています。連載中は1です。|\n",
        "|isstop | 長期連載停止中なら1、それ以外は0です。|\n",
        "|isr15 | 登録必須キーワードに「R15」が含まれる場合は1、それ以外は0です。|\n",
        "|isbl | 登録必須キーワードに「ボーイズラブ」が含まれる場合は1、それ以外は0です。|\n",
        "|isgl | 登録必須キーワードに「ガールズラブ」が含まれる場合は1、それ以外は0です。|\n",
        "|iszankoku | 登録必須キーワードに「残酷な描写あり」が含まれる場合は1、それ以外は0です。|\n",
        "|istensei | 登録必須キーワードに「異世界転生」が含まれる場合は1、それ以外は0です。|\n",
        "|istenni | 登録必須キーワードに「異世界転移」が含まれる場合は1、それ以外は0です。|\n",
        "|pc_or_k | 1はケータイのみ、2はPCのみ、3はPCとケータイで投稿された作品です。<br>対象は投稿と次話投稿時のみで、どの端末で執筆されたかを表すものではありません。|\n",
        "|fav_novel_cnt_bin | ブックマーク度|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Pk7P2OOfKo"
      },
      "source": [
        "ページ上部の「ランタイム」>「ランタイムのタイプを変更」から「GPU」「ハイメモリ」を選択"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33YmziVpXGJN"
      },
      "source": [
        "ドライブをマウント（左側サイドバーから実行）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMn4rRHdkEaH"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXBsDVH6SNHL"
      },
      "source": [
        "package install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zptsqY9EnRid"
      },
      "source": [
        "!pip install xfeat >> /dev/null\n",
        "!pip install ginza ja-ginza >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xAKRdlLnoQy"
      },
      "source": [
        "ランタイムを再起動（再起動しないとja_ginzaのloadでエラー）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow9bEgV4nm5z"
      },
      "source": [
        "import json\n",
        "import pickle\n",
        "import re\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "import regex\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import lightgbm as lgb\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('ja_ginza')\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "from xfeat import Pipeline, SelectCategorical, LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h-7Ey13Aie4"
      },
      "source": [
        "### ファイル読み込み・データ確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ-5c2b-xnFZ"
      },
      "source": [
        "# train.csv, test.csv, sample_submission.csvを格納しているパスを指定ください\n",
        "PATH = 'drive/MyDrive/Nishika/Narou/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCtJirBCnr_M"
      },
      "source": [
        "cd {PATH}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBHvR61HoDXy"
      },
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "sub_df = pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNwT6C5DoD_p",
        "collapsed": true
      },
      "source": [
        "raw_df = pd.concat([train_df, test_df])\n",
        "train_idx = train_df.shape[0] # 何行目までが学習データか、後ほど使う\n",
        "print(raw_df.shape)\n",
        "raw_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmsjlQWJ6Pbn"
      },
      "source": [
        "今回与えられているデータの件数は、学習用・評価用合わせて4万件強で、小説家になろうに掲載されている90万件ほどの作品の一部となっています。<br>\n",
        "目的変数であるブックマーク度の分布を確認してみます。ついでに、冒頭で主観的な仮説を出したタイトル・あらすじの長さについても見てみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnpEGkEKz8M8"
      },
      "source": [
        "TARGET = 'fav_novel_cnt_bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw2_JNk2z5SU"
      },
      "source": [
        "# ブックマーク度の分布を確認\n",
        "\n",
        "plt.hist(train_df[TARGET], range=(-0.5, 4.5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEPobeBJ_MD9"
      },
      "source": [
        "ブックマーク度が高い（ブックマーク数が多い）作品はかなり限定的です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k66PpGXjxuqH"
      },
      "source": [
        "# タイトルの長さとブックマーク度を箱髭図にプロット\n",
        "\n",
        "data = []\n",
        "xlabels = []\n",
        "\n",
        "for t in sorted(train_df[TARGET].unique()):\n",
        "    xlabels.append(t)\n",
        "    tmp = train_df[train_df[TARGET] == t]\n",
        "    data.append(tmp['title'].map(len))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.boxplot(data)\n",
        "plt.xlabel(TARGET)\n",
        "plt.ylabel('len_title')\n",
        "ax = plt.gca()\n",
        "plt.setp(ax, xticklabels=xlabels)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEKkJnca0n6k"
      },
      "source": [
        "タイトルが短くまとまっている方が良作品、という冒頭の仮説はそうでもないことがわかってしまいました。<br>\n",
        "そう言えば、自分は[ティアムーン帝国物語　～断頭台から始まる、姫の転生逆転ストーリー～](https://ncode.syosetu.com/n8920ex/)や[悲劇の元凶となる最強外道ラスボス女王は民の為に尽くします。〜ラスボスチートと王女の権威で救える人は救いたい〜](https://ncode.syosetu.com/n0692es/)も好きなことを思い出したので、前言撤回します。\n",
        "\n",
        "しかし、予測に有効な可能性はもちろん残っています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-xR2rt30zQ1"
      },
      "source": [
        "# あらすじの長さとブックマーク度を箱髭図にプロット\n",
        "\n",
        "data = []\n",
        "xlabels = []\n",
        "\n",
        "for t in sorted(train_df[TARGET].unique()):\n",
        "    xlabels.append(t)\n",
        "    tmp = train_df[train_df[TARGET] == t]\n",
        "    data.append(tmp['story'].map(len))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.boxplot(data)\n",
        "plt.xlabel(TARGET)\n",
        "plt.ylabel('len_story')\n",
        "ax = plt.gca()\n",
        "plt.setp(ax,xticklabels = xlabels)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQUk9PpA1gp8"
      },
      "source": [
        "あらすじの長い方がブックマーク度が高い、という関係があるようです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJLzVatqBAT_"
      },
      "source": [
        "### feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS7Mh_8FXQGK"
      },
      "source": [
        "構造化データを使った特徴量作成については、今回はごく簡単なものにとどめます。<br>\n",
        "xfeatは様々な特徴量生成が可能なライブラリですので、是非使い込んでみてください！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Df2IIXoOje",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import datetime\n",
        "\n",
        "dt_now = datetime.datetime.now()\n",
        "raw_df['past_days'] = raw_df['general_firstup'].apply(lambda x: (dt_now - datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')).days)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvlscU3Nn53H"
      },
      "source": [
        "raw_df['keyword'].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDyLC24Asm1B",
        "collapsed": true
      },
      "source": [
        "encoder = Pipeline([\n",
        "    SelectCategorical(),\n",
        "    LabelEncoder(output_suffix=\"\"),\n",
        "])\n",
        "\n",
        "le_df = encoder.fit_transform(raw_df)\n",
        "le_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNPqtsRTsozC"
      },
      "source": [
        "raw_df['writer'] = le_df['writer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roYMzEXspF9r",
        "collapsed": true
      },
      "source": [
        "raw_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO-5afmPBBny"
      },
      "source": [
        "#### テキストから古典的な特徴量作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nMd8hpQXkkt"
      },
      "source": [
        "まずは品詞ベースで形態素の数をカウント、全体に占める割合を計算した特徴量を作成します。<br>\n",
        "さらに、タイトルなどに否定系の表現がどの程度入っているかも影響がありそうと考え、否定系の助動詞に絞って同様の計算をしてみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrDaoR9gswQg"
      },
      "source": [
        "def create_pos_features(texts):\n",
        "    \"\"\"品詞ベースの特徴量を作成\"\"\"\n",
        "\n",
        "    docs = list(tqdm(nlp.pipe(texts, disable=['ner'])))\n",
        "\n",
        "    pos_data = {}\n",
        "\n",
        "    negative_auxs = [0]*len(docs)\n",
        "\n",
        "    for i, doc in enumerate(docs):\n",
        "        for token in doc:\n",
        "            # 品詞が○○である形態素の数。後で全形態素数で割る\n",
        "            if token.pos_ not in pos_data:\n",
        "                pos_data[token.pos_] = [0]*len(docs)\n",
        "            pos_data[token.pos_][i] += 1\n",
        "\n",
        "            # 品詞が助動詞の「ない」「ぬ」「ん」の数。後で全形態素数で割る\n",
        "            if token.pos_ == 'AUX' and token.lemma_ in ['ない', 'ぬ', 'ん']:\n",
        "                negative_auxs[i] += 1\n",
        "\n",
        "    pos_df = pd.DataFrame.from_dict(pos_data, orient='index').T\n",
        "    pos_df['num_token'] = pos_df.sum(axis=1) # 全形態素数\n",
        "\n",
        "    pos_df['NEG_AUX'] = negative_auxs\n",
        "\n",
        "    for colname in pos_df.columns:\n",
        "        if colname != 'num_token':\n",
        "            pos_df[colname] /= pos_df['num_token'] # 全形態素数で割る\n",
        "\n",
        "    return pos_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg-AE7ZnX8WC"
      },
      "source": [
        "単純なテキストの長さとともに、ひらがなやカタカナ、漢字、絵文字などがどの程度含まれるか、も何らか影響があるかもしれません。<br>\n",
        "文字種別に文字数をカウントし、特徴量としてみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcK-UHLtswX"
      },
      "source": [
        "def create_type_features(texts):\n",
        "    \"\"\"文字種ベースの特徴量を作成\"\"\"\n",
        "\n",
        "    type_data = []\n",
        "\n",
        "    for text in texts:\n",
        "        tmp = []\n",
        "        \n",
        "        tmp.append(len(text))\n",
        "\n",
        "        # 平仮名の文字数カウント\n",
        "        p = re.compile('[\\u3041-\\u309F]+')\n",
        "        s = ''.join(p.findall(text))\n",
        "        tmp.append(len(s))\n",
        "\n",
        "        # カタカナの文字数カウント\n",
        "        p = re.compile('[\\u30A1-\\u30FF]+')\n",
        "        s = ''.join(p.findall(text))\n",
        "        tmp.append(len(s))\n",
        "\n",
        "        # 漢字の文字数カウント\n",
        "        p = regex.compile(r'\\p{Script=Han}+')\n",
        "        s = ''.join(p.findall(text))\n",
        "        tmp.append(len(s))\n",
        "\n",
        "        # 絵文字の文字数カウント\n",
        "        p = regex.compile(r'\\p{Emoji_Presentation=Yes}+')\n",
        "        s = ''.join(p.findall(text))\n",
        "        tmp.append(len(s))\n",
        "\n",
        "        type_data.append(tmp)\n",
        "\n",
        "    colnames = ['length', 'hiragana_length', 'katakana_length', 'kanji_length', 'emoji_length']\n",
        "    type_df = pd.DataFrame(type_data, columns=colnames)\n",
        "\n",
        "    for colname in type_df.columns:\n",
        "        if colname != 'length':\n",
        "            type_df[colname] /= type_df['length']\n",
        "\n",
        "    return type_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wpx62692dgn"
      },
      "source": [
        "まずタイトルに対して処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89aOlJ-_stD9"
      },
      "source": [
        "titles = raw_df['title'].values\n",
        "docs = list(nlp.pipe(titles, disable=['ner']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C8SP-S3t_by"
      },
      "source": [
        "title_pos_df = create_pos_features(titles)\n",
        "title_pos_df.columns = ['title_' + colname for colname in title_pos_df.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9KYuduM0BQq"
      },
      "source": [
        "title_type_df = create_type_features(titles)\n",
        "title_type_df.columns = ['title_' + colname for colname in title_type_df.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nV5uiI52fmN"
      },
      "source": [
        "次にあらすじに対して処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H91xAkPhuXn5"
      },
      "source": [
        "stories = raw_df['story'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ih8pWxducp2"
      },
      "source": [
        "# かなり大きなメモリを使用するので、念の為データを分割しバッチ処理\n",
        "\n",
        "nrow_one_loop = 20000\n",
        "nloop = np.floor(len(stories)/nrow_one_loop)\n",
        "min_idx = 0\n",
        "\n",
        "story_pos_dfs = []\n",
        "\n",
        "while min_idx < len(stories):\n",
        "    tmp_stories = stories[min_idx:min_idx+nrow_one_loop]\n",
        "    suffix = str(min_idx//nrow_one_loop) if len(str(min_idx//nrow_one_loop)) != 1 else '0'+str(min_idx//nrow_one_loop)\n",
        "    story_pos_dfs.append(create_pos_features(tmp_stories))\n",
        "    min_idx += nrow_one_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqVYNoWUutSk"
      },
      "source": [
        "story_pos_df = pd.concat(story_pos_dfs)\n",
        "del story_pos_dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipIvE0n7u0GP"
      },
      "source": [
        "# かなり大きなメモリを使用するので、念の為データを分割しバッチ処理\n",
        "\n",
        "nrow_one_loop = 20000\n",
        "nloop = np.floor(len(stories)/nrow_one_loop)\n",
        "min_idx = 0\n",
        "\n",
        "story_type_dfs = []\n",
        "\n",
        "while min_idx < len(stories):\n",
        "    tmp_stories = stories[min_idx:min_idx+nrow_one_loop]\n",
        "    story_type_dfs.append(create_type_features(tmp_stories))\n",
        "    min_idx += nrow_one_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8PkLKa7u1go"
      },
      "source": [
        "story_type_df = pd.concat(story_type_dfs)\n",
        "del story_type_dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNtl_ln8-_i_"
      },
      "source": [
        "#### BERTVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fKTRMdhYkxc"
      },
      "source": [
        "上では古典的な特徴量作成を行いましたが、より近代的な方法も試してみましょう。<br>\n",
        "BERTを使って、文章をベクトル表現に変換してみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kQUmXyw-8x9"
      },
      "source": [
        "!pip install transformers fugashi ipadic >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oac8i6BT-9zQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from transformers import BertJapaneseTokenizer\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k409So9l_PNv"
      },
      "source": [
        "class BertSequenceVectorizer:\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
        "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(self.model_name)\n",
        "        self.bert_model = transformers.BertModel.from_pretrained(self.model_name)\n",
        "        self.bert_model = self.bert_model.to(self.device)\n",
        "        self.max_len = 128\n",
        "            \n",
        "\n",
        "    def vectorize(self, sentence : str) -> np.array:\n",
        "        inp = self.tokenizer.encode(sentence)\n",
        "        len_inp = len(inp)\n",
        "\n",
        "        if len_inp >= self.max_len:\n",
        "            inputs = inp[:self.max_len]\n",
        "            masks = [1] * self.max_len\n",
        "        else:\n",
        "            inputs = inp + [0] * (self.max_len - len_inp)\n",
        "            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n",
        "\n",
        "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n",
        "        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n",
        "        \n",
        "        seq_out = self.bert_model(inputs_tensor, masks_tensor)[0]\n",
        "        pooled_out = self.bert_model(inputs_tensor, masks_tensor)[1]\n",
        "\n",
        "        if torch.cuda.is_available():    \n",
        "            return seq_out[0][0].cpu().detach().numpy() # 0番目は [CLS] token, 768 dim の文章特徴量\n",
        "        else:\n",
        "            return seq_out[0][0].detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk5ASG89_QHK"
      },
      "source": [
        "BSV = BertSequenceVectorizer()\n",
        "raw_df['title_feature'] = raw_df['title'].progress_apply(lambda x: BSV.vectorize(x) if x is not np.nan else np.array([0]*768))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VOHI0Fz_aLe"
      },
      "source": [
        "title_bert_df = pd.DataFrame(raw_df['title_feature'].tolist())\n",
        "title_bert_df.columns = ['title_bertvec_'+str(col) for col in title_bert_df.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB0khNTO_aTQ"
      },
      "source": [
        "BSV = BertSequenceVectorizer()\n",
        "raw_df['story_feature'] = raw_df['story'].progress_apply(lambda x: BSV.vectorize(x) if x is not np.nan else np.array([0]*768))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXue8-_B_gP_"
      },
      "source": [
        "story_bert_df = pd.DataFrame(raw_df['story_feature'].tolist())\n",
        "story_bert_df.columns = ['story_bertvec_'+str(col) for col in story_bert_df.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sDpa_otWFMg"
      },
      "source": [
        "import pickle\n",
        "raw_df.to_pickle('./data/dst/raw_df.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cc_C9CLWckP"
      },
      "source": [
        "raw_df = pd.read_pickle('./data/dst/raw_df.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzG6HHZlqbHn"
      },
      "source": [
        "title_bert_df = pd.DataFrame(raw_df['title_feature'].tolist())\n",
        "title_bert_df.columns = ['title_bertvec_'+str(col) for col in title_bert_df.columns]\n",
        "story_bert_df = pd.DataFrame(raw_df['story_feature'].tolist())\n",
        "story_bert_df.columns = ['story_bertvec_'+str(col) for col in story_bert_df.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiH1iow__Ez2"
      },
      "source": [
        "### DataFrame結合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm6IRwRy7eA1"
      },
      "source": [
        "raw_df.reset_index(drop=True, inplace=True)\n",
        "#title_pos_df.reset_index(drop=True, inplace=True)\n",
        "#title_type_df.reset_index(drop=True, inplace=True)\n",
        "#story_pos_df.reset_index(drop=True, inplace=True)\n",
        "#story_type_df.reset_index(drop=True, inplace=True)\n",
        "title_bert_df.reset_index(drop=True, inplace=True)\n",
        "story_bert_df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtPZP9Ckv0SN"
      },
      "source": [
        "#concat_df = pd.concat([raw_df, title_pos_df, title_type_df, story_pos_df, story_type_df, title_bert_df, story_bert_df], axis=1)\n",
        "concat_df = pd.concat([raw_df, title_bert_df, story_bert_df], axis=1)\n",
        "concat_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrVqlHSkj7Ie"
      },
      "source": [
        "concat_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXiP2IUW_JHO"
      },
      "source": [
        "### データ分割、学習、評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azm30VLw23Jh"
      },
      "source": [
        "LightGBMで学習、評価します。バリデーションはHold-out法を採用しています"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aykUAVPFwGjI"
      },
      "source": [
        "# カテゴリ変数・連続変数各々のカラムを特定\n",
        "\n",
        "cat_cols = ['userid', 'writer', 'biggenre', 'genre', 'novel_type', 'end', 'isstop', 'isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni', 'pc_or_k']\n",
        "\n",
        "#num_cols = ['past_days']\n",
        "#num_cols += list(title_pos_df.columns) + list(title_type_df.columns) + list(story_pos_df.columns) + list(story_type_df.columns) + list(title_bert_df.columns) + list(story_bert_df.columns)\n",
        "num_cols = []\n",
        "num_cols += list(title_bert_df.columns) + list(story_bert_df.columns)\n",
        "\n",
        "feat_cols = cat_cols + num_cols\n",
        "\n",
        "ID = 'ncode'\n",
        "TARGET = 'fav_novel_cnt_bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eogikT0wLYp"
      },
      "source": [
        "#@title\n",
        "# バリデーションはHold-out法（一定割合で学習データと評価データの2つに分割）で行う\n",
        "\n",
        "#train_df = concat_df.iloc[:35000, :]\n",
        "#val_df = concat_df.iloc[35000:train_idx, :]\n",
        "test_df = concat_df.iloc[train_idx:, :]\n",
        "#print(train_df.shape, val_df.shape, test_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnYQK_U0wYgG",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "train_x = train_df[feat_cols]\n",
        "train_y = train_df[TARGET]\n",
        "val_x = val_df[feat_cols]\n",
        "val_y = val_df[TARGET]\n",
        "test_x = test_df[feat_cols]\n",
        "test_y = test_df[TARGET]\n",
        "train_x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7E2a5HHlmUf"
      },
      "source": [
        "X = concat_df.iloc[:train_idx, :][feat_cols]\n",
        "y = concat_df.iloc[:train_idx, :][TARGET]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAxbcwYeZ3fo"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXK-eLqOZ3fp"
      },
      "source": [
        "def fit(tr_x, tr_y, va_x, va_y, tr_w=None, va_w=None):\n",
        "    \"\"\"\n",
        "    model training\n",
        "  \n",
        "    Parameters\n",
        "    ----------\n",
        "    tr_x: pd.DataFrame\n",
        "    tr_y: pd.DataFrame\n",
        "    va_x: pd.DataFrame\n",
        "    va_y: pd.DataFrame\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    model:\n",
        "        - 学習済みモデル\n",
        "    va_pred: \n",
        "        - 検証データの予測結果\n",
        "    \"\"\" \n",
        "    # パラメータの設定\n",
        "    SEED = 0\n",
        "\n",
        "    params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_classes': 5,\n",
        "        'metric': 'multi_logloss',\n",
        "        'num_leaves': 42,\n",
        "        'max_depth': 7,\n",
        "        \"feature_fraction\": 0.8,\n",
        "        'subsample_freq': 1,\n",
        "        \"bagging_fraction\": 0.95,\n",
        "        'min_data_in_leaf': 2,\n",
        "        'learning_rate': 0.1,\n",
        "        \"boosting\": \"gbdt\",\n",
        "        \"lambda_l1\": 0.1,\n",
        "        \"lambda_l2\": 10,\n",
        "        \"verbosity\": -1,\n",
        "        \"random_state\": 42,\n",
        "        \"num_boost_round\": 50000,\n",
        "        \"early_stopping_rounds\": 100\n",
        "    }\n",
        "\n",
        "    # 学習セットを作成\n",
        "    lgb_train = lgb.Dataset(tr_x, tr_y)\n",
        "    lgb_eval = lgb.Dataset(va_x, va_y, reference=lgb_train)\n",
        "\n",
        "    # モデルの学習\n",
        "    '''\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_set=lgb_train, # トレーニングデータの指定\n",
        "        valid_sets=[lgb_train, lgb_eval],\n",
        "        valid_names=['train', 'valid'],\n",
        "        num_boost_round=1000,\n",
        "        early_stopping_rounds = 100,\n",
        "        verbose_eval = 20\n",
        "        )\n",
        "    '''\n",
        "    \n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        lgb_train, \n",
        "        categorical_feature = cat_cols,\n",
        "        valid_names = ['train', 'valid'],\n",
        "        valid_sets =[lgb_train, lgb_eval], \n",
        "        verbose_eval = 100,\n",
        "        )\n",
        "    \n",
        "    # 検証データの予測確率\n",
        "    va_pred = model.predict(va_x)\n",
        "\n",
        "    return model, va_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ0h1X3XZ3fp"
      },
      "source": [
        "def scoring(y_true, y_prob):\n",
        "    \"\"\"Multi-class logloss\"\"\"\n",
        "    return log_loss(y_true, y_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdhiyVGuq762",
        "collapsed": true
      },
      "source": [
        "X['userid']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH7izbOmZ3fq",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# 値が全て同じカラムとobject型のカラムは使用しない\n",
        "drop_lst = ['end', 'isstop']\n",
        "object_lst = ['ncode', 'general_firstup', 'title', 'story', 'keyword', 'writer']\n",
        "\n",
        "df_train_numeric = df_train.drop(drop_lst, axis=1)\n",
        "df_train_numeric = df_train_numeric.drop(object_lst, axis=1)\n",
        "\n",
        "df_test_numeric = df_test.drop(drop_lst, axis=1)\n",
        "df_test_numeric = df_test_numeric.drop(object_lst, axis=1)\n",
        "\n",
        "df_train_numeric.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM4ZdmcDZ3fq",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# 説明変数,目的変数を分割\n",
        "X = df_train_numeric.drop('fav_novel_cnt_bin', axis=1)\n",
        "y = df_train_numeric['fav_novel_cnt_bin']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAcpeecgcTpL",
        "collapsed": true
      },
      "source": [
        "df_pred = pd.DataFrame(index=X.index, columns=['proba_0', 'proba_1', 'proba_2',\t'proba_3', 'proba_4'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LV_MpTK35J5"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nWJeDHl4J4A"
      },
      "source": [
        "!nvidia-smi -pm 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhuYCXEYZNDY"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "models = []\n",
        "df_pred = pd.DataFrame(index=X.index, columns=['proba_0', 'proba_1', 'proba_2',\t'proba_3', 'proba_4'])\n",
        "\n",
        "# トレーニングデータ,テストデータの分割\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "for i, (train_index, valid_index) in enumerate(skf.split(X, y), 1):\n",
        "    print(f'---CV{i}---')\n",
        "    X_train, y_train = X.loc[train_index], y.loc[train_index]\n",
        "    X_valid, y_valid = X.loc[valid_index], y.loc[valid_index]\n",
        "    # モデルの学習\n",
        "    model, va_pred = fit(X_train, y_train, X_valid, y_valid) \n",
        "    # モデルの格納\n",
        "    models.append(model)\n",
        "    # 検証データの予測結果を格納\n",
        "    df_pred.loc[valid_index] = va_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVOdGaXFQOvf"
      },
      "source": [
        "# モデルの保存\n",
        "import pickle\n",
        "\n",
        "# カレントディレクトリへモデルを保存\n",
        "file = 'trained_model_20211115.pkl'\n",
        "pickle.dump(models, open(file, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OqjaOtHQhRa"
      },
      "source": [
        "# モデルの削除\n",
        "del models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pMC5FCQXwh"
      },
      "source": [
        "models = pickle.load(open('trained_model_20211115.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM1LKgIPZ3fr"
      },
      "source": [
        "# CVスコア\n",
        "scoring(df_train['fav_novel_cnt_bin'], df_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSVcm75gBkSL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzjmt8JdEDYU"
      },
      "source": [
        "### 推論・投稿ファイル作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdC3uT2QEDYV"
      },
      "source": [
        "for i in range(len(models)):\n",
        "    model = models[i]\n",
        "    test_pred = model.predict(test_df, num_iteration=model.best_iteration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjtPXPH-EDYV"
      },
      "source": [
        "sub_df.iloc[:, 1:] = test_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bi8JA_KEDYV"
      },
      "source": [
        "sub_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu9Ard9lEDYV"
      },
      "source": [
        "sub_df.to_csv('./output/test_submission2.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTQyX4FJEDYV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZSYd9d8Z3fr"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjZ8avBCZ3fr"
      },
      "source": [
        "def predict(models, x):\n",
        "    \"\"\"\n",
        "    prediction\n",
        "  \n",
        "    Parameters\n",
        "    ----------\n",
        "    models: list\n",
        "        - trained model\n",
        "    x: pd.DataFrame\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    result: pd.DataFrame\n",
        "    \"\"\" \n",
        "    result = pd.DataFrame(0, index=x.index, columns=['proba_0',\t'proba_1', 'proba_2', 'proba_3', 'proba_4'])\n",
        "    for model in models:\n",
        "        pred_prob = model.predict(x, num_iteration=model.best_iteration)\n",
        "        lgb.plot_importance(model, figsize=(12,8), max_num_features=50, importance_type='gain')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        df_pred = pd.DataFrame(pred_prob, index=x.index, columns=['proba_0', 'proba_1', 'proba_2', 'proba_3', 'proba_4'])\n",
        "        result += df_pred\n",
        "    result = result / 5\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLuMcllevXwH"
      },
      "source": [
        "# title\n",
        "models = []\n",
        "df_pred = pd.DataFrame(index=X.index, columns=['proba_0', 'proba_1', 'proba_2',\t'proba_3', 'proba_4'])\n",
        "\n",
        "# トレーニングデータ,テストデータの分割\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "for i, (train_index, valid_index) in enumerate(skf.split(X, y), 1):\n",
        "    print('---CV{i}---')\n",
        "    # X_train, y_train = X.loc[train_index], y.loc[train_index]\n",
        "    # X_valid, y_valid = X.loc[valid_index], y.loc[valid_index]\n",
        "    print(len(train_story[train_index]), len(y.loc[train_index]))\n",
        "    X_train, y_train = train_story[train_index], y.loc[train_index]\n",
        "    X_valid, y_valid = train_story[valid_index], y.loc[valid_index]\n",
        "    # モデルの学習\n",
        "    model, va_pred = fit(X_train, y_train, X_valid, y_valid) \n",
        "    # モデルの格納\n",
        "    models.append(model)\n",
        "    # 検証データの予測結果を格納\n",
        "    df_pred.loc[valid_index] = va_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRohoVTSRBqi"
      },
      "source": [
        "result = predict(models, test_df)\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30f8SFD_Z3fs"
      },
      "source": [
        "## Create Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdAtMhV_Z3fs"
      },
      "source": [
        "output_fpath = Path('./')\n",
        "submission.iloc[:, 1:] = result\n",
        "submission.to_csv(output_fpath / 'submission.csv', header=True, index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ5pxAc5opde"
      },
      "source": [
        "# Modelingその2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5ZL1iv0waUU"
      },
      "source": [
        "#@title\n",
        "# 評価指標はMulti-class logloss\n",
        "\n",
        "SEED = 0\n",
        "\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_classes': 5,\n",
        "    'metric': 'multi_logloss',\n",
        "    'num_leaves': 42,\n",
        "    'max_depth': 7,\n",
        "    \"feature_fraction\": 0.8,\n",
        "    'subsample_freq': 1,\n",
        "    \"bagging_fraction\": 0.95,\n",
        "    'min_data_in_leaf': 2,\n",
        "    'learning_rate': 0.1,\n",
        "    \"boosting\": \"gbdt\",\n",
        "    \"lambda_l1\": 0.1,\n",
        "    \"lambda_l2\": 10,\n",
        "    \"verbosity\": -1,\n",
        "    \"random_state\": 42,\n",
        "    \"num_boost_round\": 50000,\n",
        "    \"early_stopping_rounds\": 100\n",
        "}\n",
        "\n",
        "train_data = lgb.Dataset(train_x, label=train_y)\n",
        "val_data = lgb.Dataset(val_x, label=val_y)\n",
        "\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data, \n",
        "    categorical_feature = cat_cols,\n",
        "    valid_names = ['train', 'valid'],\n",
        "    valid_sets =[train_data, val_data], \n",
        "    verbose_eval = 100,\n",
        ")\n",
        "\n",
        "val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
        "\n",
        "pred_df = pd.DataFrame(sorted(zip(val_x.index, val_pred, val_y)), columns=['index', 'predict', 'actual'])\n",
        "\n",
        "feature_imp = pd.DataFrame(sorted(zip(model.feature_importance(), train_x.columns)), columns=['importance', 'feature'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKinixVQWwof"
      },
      "source": [
        "file = './models/trained_model_20211110.pkl'\n",
        "pickle.dump(model, open(file, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGBi4u44wfmx"
      },
      "source": [
        "# 特徴量の重要度を確認\n",
        "\n",
        "lgb.plot_importance(model, figsize=(12,8), max_num_features=50, importance_type='gain')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvt8U04USQAJ"
      },
      "source": [
        "val_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6mBxK-280dN"
      },
      "source": [
        "# 評価指標はlog lossだが、accuracyも見てみる\n",
        "\n",
        "val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
        "val_pred_max = np.argmax(val_pred, axis=1)  # 最尤と判断したクラスの値にする\n",
        "accuracy = sum(val_y == val_pred_max) / len(val_y)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MkguIKuNoyj"
      },
      "source": [
        "### 推論・投稿ファイル作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhNz-Xgjwi6N"
      },
      "source": [
        "test_pred = model.predict(test_x, num_iteration=model.best_iteration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F34yd7TT89v2"
      },
      "source": [
        "sub_df.iloc[:, 1:] = test_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsiWVECZRwPo"
      },
      "source": [
        "sub_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYnLRpCm8-xb"
      },
      "source": [
        "sub_df.to_csv('./output/test_submission2.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGMcB63-9JJT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}